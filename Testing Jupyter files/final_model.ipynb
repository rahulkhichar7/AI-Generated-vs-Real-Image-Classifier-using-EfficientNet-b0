{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfa81c9",
   "metadata": {},
   "source": [
    "Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8812a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d941a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset from folders\n",
    "def build_dataframe():\n",
    "    data = []\n",
    "    for label, folder in enumerate(['ai', 'real']):\n",
    "        n = 0\n",
    "        for file in os.listdir(folder):\n",
    "            if n == 20000:\n",
    "                break\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.webp', '.gif')):\n",
    "                n += 1\n",
    "                data.append({'file_name': os.path.join(folder, file), 'label': label})\n",
    "    df = pd.DataFrame(data)\n",
    "    return train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "train_df, val_df = build_dataframe()\n",
    "test_files = os.listdir('test_data/teamspace/studios/this_studio/final_test_renamed')\n",
    "test_df = pd.DataFrame({'file_name': [f'test_data/teamspace/studios/this_studio/final_test_renamed/{x}' for x in test_files]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f463ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, is_test=False):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['file_name']\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.is_test:\n",
    "            return image, -1\n",
    "        else:\n",
    "            label = int(self.df.iloc[idx]['label'])\n",
    "            return image, label\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5496a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters from Optuna\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.000762\n",
    "WEIGHT_DECAY = 0.0003804\n",
    "\n",
    "# DataLoaders (⬅️ Made memory-efficient: num_workers=2 and pin_memory=True)\n",
    "train_dataset = ImageDataset(train_df, transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, transform=test_transform)\n",
    "test_dataset = ImageDataset(test_df, transform=test_transform, is_test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55ac0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_16944\\1553475832.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_dummy.pth', map_location=device))  # ⬅️ Load from given path\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = efficientnet_b0(weights=None)  # ⬅️ No default weights since we are loading our own\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model_dummy.pth', map_location=device))  # ⬅️ Load from given path\n",
    "model = model.to(device)\n",
    "model.eval() \n",
    "\n",
    "# Freeze all layers except last few (⬅️ Same)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.features[-3:].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f194884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping (⬅️ Added Early Stopping logic)\n",
    "best_val_acc = 0\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(20):\n",
    "    print(f\"Epoch {epoch} started.\")\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    cnt=0\n",
    "    for images, labels in train_loader:\n",
    "        cnt+=1\n",
    "        print(f\"Epoch {epoch}, count: {cnt}\")\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    print(\"  Evaluating the model\")\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    print(f\"Epoch {epoch+1}: Train Acc = {train_acc:.2f}%, Val Acc = {val_acc:.2f}%\")\n",
    "    scheduler.step()\n",
    "\n",
    "    # Free up unused memory (⬅️ Added)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.save(model.state_dict(), f'model__{epoch}_{val_acc}_{train_acc}.pth')\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"⛔ Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load best model for inference\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Inference on test data\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['file_name'].apply(lambda x: os.path.basename(x)),\n",
    "    'label': preds\n",
    "})\n",
    "submission.to_csv('final_submission.csv', index=False)\n",
    "print(\"✅ final_submission.csv saved successfully!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63998263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_15336\\924277021.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_2.pth', map_location=device))  # ⬅️ Load from given path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epch no 0\n",
      "epoch 0 sub ep 1\n",
      "epoch 0 sub ep 2\n",
      "epoch 0 sub ep 3\n",
      "epoch 0 sub ep 4\n",
      "epoch 0 sub ep 5\n",
      "epoch 0 sub ep 6\n",
      "epoch 0 sub ep 7\n",
      "epoch 0 sub ep 8\n",
      "epoch 0 sub ep 9\n",
      "epoch 0 sub ep 10\n",
      "epoch 0 sub ep 11\n",
      "epoch 0 sub ep 12\n",
      "epoch 0 sub ep 13\n",
      "epoch 0 sub ep 14\n",
      "epoch 0 sub ep 15\n",
      "epoch 0 sub ep 16\n",
      "epoch 0 sub ep 17\n",
      "epoch 0 sub ep 18\n",
      "epoch 0 sub ep 19\n",
      "epoch 0 sub ep 20\n",
      "epoch 0 sub ep 21\n",
      "epoch 0 sub ep 22\n",
      "epoch 0 sub ep 23\n",
      "epoch 0 sub ep 24\n",
      "epoch 0 sub ep 25\n",
      "epoch 0 sub ep 26\n",
      "epoch 0 sub ep 27\n",
      "epoch 0 sub ep 28\n",
      "epoch 0 sub ep 29\n",
      "epoch 0 sub ep 30\n",
      "epoch 0 sub ep 31\n",
      "epoch 0 sub ep 32\n",
      "epoch 0 sub ep 33\n",
      "epoch 0 sub ep 34\n",
      "epoch 0 sub ep 35\n",
      "epoch 0 sub ep 36\n",
      "epoch 0 sub ep 37\n",
      "epoch 0 sub ep 38\n",
      "epoch 0 sub ep 39\n",
      "epoch 0 sub ep 40\n",
      "epoch 0 sub ep 41\n",
      "epoch 0 sub ep 42\n",
      "epoch 0 sub ep 43\n",
      "epoch 0 sub ep 44\n",
      "epoch 0 sub ep 45\n",
      "epoch 0 sub ep 46\n",
      "epoch 0 sub ep 47\n",
      "epoch 0 sub ep 48\n",
      "epoch 0 sub ep 49\n",
      "epoch 0 sub ep 50\n",
      "epoch 0 sub ep 51\n",
      "epoch 0 sub ep 52\n",
      "epoch 0 sub ep 53\n",
      "epoch 0 sub ep 54\n",
      "epoch 0 sub ep 55\n",
      "epoch 0 sub ep 56\n",
      "epoch 0 sub ep 57\n",
      "epoch 0 sub ep 58\n",
      "epoch 0 sub ep 59\n",
      "epoch 0 sub ep 60\n",
      "epoch 0 sub ep 61\n",
      "epoch 0 sub ep 62\n",
      "epoch 0 sub ep 63\n",
      "epoch 0 sub ep 64\n",
      "epoch 0 sub ep 65\n",
      "epoch 0 sub ep 66\n",
      "epoch 0 sub ep 67\n",
      "epoch 0 sub ep 68\n",
      "epoch 0 sub ep 69\n",
      "epoch 0 sub ep 70\n",
      "epoch 0 sub ep 71\n",
      "epoch 0 sub ep 72\n",
      "epoch 0 sub ep 73\n",
      "epoch 0 sub ep 74\n",
      "epoch 0 sub ep 75\n",
      "epoch 0 sub ep 76\n",
      "epoch 0 sub ep 77\n",
      "epoch 0 sub ep 78\n",
      "epoch 0 sub ep 79\n",
      "epoch 0 sub ep 80\n",
      "epoch 0 sub ep 81\n",
      "epoch 0 sub ep 82\n",
      "epoch 0 sub ep 83\n",
      "epoch 0 sub ep 84\n",
      "epoch 0 sub ep 85\n",
      "epoch 0 sub ep 86\n",
      "epoch 0 sub ep 87\n",
      "epoch 0 sub ep 88\n",
      "epoch 0 sub ep 89\n",
      "epoch 0 sub ep 90\n",
      "epoch 0 sub ep 91\n",
      "epoch 0 sub ep 92\n",
      "epoch 0 sub ep 93\n",
      "epoch 0 sub ep 94\n",
      "epoch 0 sub ep 95\n",
      "epoch 0 sub ep 96\n",
      "epoch 0 sub ep 97\n",
      "epoch 0 sub ep 98\n",
      "epoch 0 sub ep 99\n",
      "epoch 0 sub ep 100\n",
      "epoch 0 sub ep 101\n",
      "epoch 0 sub ep 102\n",
      "epoch 0 sub ep 103\n",
      "epoch 0 sub ep 104\n",
      "epoch 0 sub ep 105\n",
      "epoch 0 sub ep 106\n",
      "epoch 0 sub ep 107\n",
      "epoch 0 sub ep 108\n",
      "epoch 0 sub ep 109\n",
      "epoch 0 sub ep 110\n",
      "epoch 0 sub ep 111\n",
      "epoch 0 sub ep 112\n",
      "epoch 0 sub ep 113\n",
      "epoch 0 sub ep 114\n",
      "epoch 0 sub ep 115\n",
      "epoch 0 sub ep 116\n",
      "epoch 0 sub ep 117\n",
      "⚠️ Skipping corrupted image: real\\10017.jpg\n",
      "epoch 0 sub ep 118\n",
      "epoch 0 sub ep 119\n",
      "epoch 0 sub ep 120\n",
      "epoch 0 sub ep 121\n",
      "epoch 0 sub ep 122\n",
      "epoch 0 sub ep 123\n",
      "epoch 0 sub ep 124\n",
      "epoch 0 sub ep 125\n",
      "epoch 0 sub ep 126\n",
      "epoch 0 sub ep 127\n",
      "epoch 0 sub ep 128\n",
      "epoch 0 sub ep 129\n",
      "epoch 0 sub ep 130\n",
      "epoch 0 sub ep 131\n",
      "epoch 0 sub ep 132\n",
      "epoch 0 sub ep 133\n",
      "epoch 0 sub ep 134\n",
      "epoch 0 sub ep 135\n",
      "epoch 0 sub ep 136\n",
      "epoch 0 sub ep 137\n",
      "epoch 0 sub ep 138\n",
      "epoch 0 sub ep 139\n",
      "epoch 0 sub ep 140\n",
      "epoch 0 sub ep 141\n",
      "epoch 0 sub ep 142\n",
      "epoch 0 sub ep 143\n",
      "epoch 0 sub ep 144\n",
      "epoch 0 sub ep 145\n",
      "epoch 0 sub ep 146\n",
      "epoch 0 sub ep 147\n",
      "epoch 0 sub ep 148\n",
      "epoch 0 sub ep 149\n",
      "epoch 0 sub ep 150\n",
      "epoch 0 sub ep 151\n",
      "epoch 0 sub ep 152\n",
      "epoch 0 sub ep 153\n",
      "epoch 0 sub ep 154\n",
      "epoch 0 sub ep 155\n",
      "epoch 0 sub ep 156\n",
      "epoch 0 sub ep 157\n",
      "epoch 0 sub ep 158\n",
      "epoch 0 sub ep 159\n",
      "epoch 0 sub ep 160\n",
      "epoch 0 sub ep 161\n",
      "epoch 0 sub ep 162\n",
      "epoch 0 sub ep 163\n",
      "epoch 0 sub ep 164\n",
      "epoch 0 sub ep 165\n",
      "epoch 0 sub ep 166\n",
      "epoch 0 sub ep 167\n",
      "epoch 0 sub ep 168\n",
      "epoch 0 sub ep 169\n",
      "epoch 0 sub ep 170\n",
      "epoch 0 sub ep 171\n",
      "epoch 0 sub ep 172\n",
      "epoch 0 sub ep 173\n",
      "epoch 0 sub ep 174\n",
      "epoch 0 sub ep 175\n",
      "epoch 0 sub ep 176\n",
      "epoch 0 sub ep 177\n",
      "epoch 0 sub ep 178\n",
      "epoch 0 sub ep 179\n",
      "epoch 0 sub ep 180\n",
      "epoch 0 sub ep 181\n",
      "epoch 0 sub ep 182\n",
      "epoch 0 sub ep 183\n",
      "epoch 0 sub ep 184\n",
      "epoch 0 sub ep 185\n",
      "epoch 0 sub ep 186\n",
      "epoch 0 sub ep 187\n",
      "epoch 0 sub ep 188\n",
      "epoch 0 sub ep 189\n",
      "epoch 0 sub ep 190\n",
      "epoch 0 sub ep 191\n",
      "epoch 0 sub ep 192\n",
      "epoch 0 sub ep 193\n",
      "epoch 0 sub ep 194\n",
      "epoch 0 sub ep 195\n",
      "epoch 0 sub ep 196\n",
      "epoch 0 sub ep 197\n",
      "epoch 0 sub ep 198\n",
      "epoch 0 sub ep 199\n",
      "epoch 0 sub ep 200\n",
      "epoch 0 sub ep 201\n",
      "epoch 0 sub ep 202\n",
      "epoch 0 sub ep 203\n",
      "epoch 0 sub ep 204\n",
      "epoch 0 sub ep 205\n",
      "epoch 0 sub ep 206\n",
      "epoch 0 sub ep 207\n",
      "epoch 0 sub ep 208\n",
      "epoch 0 sub ep 209\n",
      "epoch 0 sub ep 210\n",
      "epoch 0 sub ep 211\n",
      "epoch 0 sub ep 212\n",
      "epoch 0 sub ep 213\n",
      "epoch 0 sub ep 214\n",
      "epoch 0 sub ep 215\n",
      "epoch 0 sub ep 216\n",
      "epoch 0 sub ep 217\n",
      "epoch 0 sub ep 218\n",
      "epoch 0 sub ep 219\n",
      "epoch 0 sub ep 220\n",
      "epoch 0 sub ep 221\n",
      "epoch 0 sub ep 222\n",
      "epoch 0 sub ep 223\n",
      "epoch 0 sub ep 224\n",
      "epoch 0 sub ep 225\n",
      "epoch 0 sub ep 226\n",
      "epoch 0 sub ep 227\n",
      "epoch 0 sub ep 228\n",
      "epoch 0 sub ep 229\n",
      "epoch 0 sub ep 230\n",
      "epoch 0 sub ep 231\n",
      "epoch 0 sub ep 232\n",
      "epoch 0 sub ep 233\n",
      "epoch 0 sub ep 234\n",
      "epoch 0 sub ep 235\n",
      "epoch 0 sub ep 236\n",
      "epoch 0 sub ep 237\n",
      "epoch 0 sub ep 238\n",
      "epoch 0 sub ep 239\n",
      "epoch 0 sub ep 240\n",
      "epoch 0 sub ep 241\n",
      "epoch 0 sub ep 242\n",
      "epoch 0 sub ep 243\n",
      "epoch 0 sub ep 244\n",
      "epoch 0 sub ep 245\n",
      "epoch 0 sub ep 246\n",
      "epoch 0 sub ep 247\n",
      "epoch 0 sub ep 248\n",
      "epoch 0 sub ep 249\n",
      "epoch 0 sub ep 250\n",
      "Epoch 1: Train Acc = 90.79%, Val Acc = 92.65%\n",
      "Epch no 1\n",
      "epoch 1 sub ep 1\n",
      "epoch 1 sub ep 2\n",
      "epoch 1 sub ep 3\n",
      "epoch 1 sub ep 4\n",
      "epoch 1 sub ep 5\n",
      "epoch 1 sub ep 6\n",
      "epoch 1 sub ep 7\n",
      "epoch 1 sub ep 8\n",
      "epoch 1 sub ep 9\n",
      "epoch 1 sub ep 10\n",
      "epoch 1 sub ep 11\n",
      "epoch 1 sub ep 12\n",
      "epoch 1 sub ep 13\n",
      "epoch 1 sub ep 14\n",
      "epoch 1 sub ep 15\n",
      "epoch 1 sub ep 16\n",
      "epoch 1 sub ep 17\n",
      "epoch 1 sub ep 18\n",
      "epoch 1 sub ep 19\n",
      "epoch 1 sub ep 20\n",
      "epoch 1 sub ep 21\n",
      "epoch 1 sub ep 22\n",
      "epoch 1 sub ep 23\n",
      "epoch 1 sub ep 24\n",
      "epoch 1 sub ep 25\n",
      "epoch 1 sub ep 26\n",
      "epoch 1 sub ep 27\n",
      "epoch 1 sub ep 28\n",
      "epoch 1 sub ep 29\n",
      "epoch 1 sub ep 30\n",
      "epoch 1 sub ep 31\n",
      "epoch 1 sub ep 32\n",
      "epoch 1 sub ep 33\n",
      "epoch 1 sub ep 34\n",
      "epoch 1 sub ep 35\n",
      "epoch 1 sub ep 36\n",
      "epoch 1 sub ep 37\n",
      "epoch 1 sub ep 38\n",
      "epoch 1 sub ep 39\n",
      "epoch 1 sub ep 40\n",
      "epoch 1 sub ep 41\n",
      "epoch 1 sub ep 42\n",
      "epoch 1 sub ep 43\n",
      "epoch 1 sub ep 44\n",
      "epoch 1 sub ep 45\n",
      "epoch 1 sub ep 46\n",
      "epoch 1 sub ep 47\n",
      "epoch 1 sub ep 48\n",
      "epoch 1 sub ep 49\n",
      "epoch 1 sub ep 50\n",
      "epoch 1 sub ep 51\n",
      "epoch 1 sub ep 52\n",
      "epoch 1 sub ep 53\n",
      "epoch 1 sub ep 54\n",
      "epoch 1 sub ep 55\n",
      "epoch 1 sub ep 56\n",
      "epoch 1 sub ep 57\n",
      "epoch 1 sub ep 58\n",
      "epoch 1 sub ep 59\n",
      "epoch 1 sub ep 60\n",
      "epoch 1 sub ep 61\n",
      "epoch 1 sub ep 62\n",
      "epoch 1 sub ep 63\n",
      "epoch 1 sub ep 64\n",
      "epoch 1 sub ep 65\n",
      "epoch 1 sub ep 66\n",
      "epoch 1 sub ep 67\n",
      "epoch 1 sub ep 68\n",
      "epoch 1 sub ep 69\n",
      "epoch 1 sub ep 70\n",
      "epoch 1 sub ep 71\n",
      "⚠️ Skipping corrupted image: real\\10017.jpg\n",
      "epoch 1 sub ep 72\n",
      "epoch 1 sub ep 73\n",
      "epoch 1 sub ep 74\n",
      "epoch 1 sub ep 75\n",
      "epoch 1 sub ep 76\n",
      "epoch 1 sub ep 77\n",
      "epoch 1 sub ep 78\n",
      "epoch 1 sub ep 79\n",
      "epoch 1 sub ep 80\n",
      "epoch 1 sub ep 81\n",
      "epoch 1 sub ep 82\n",
      "epoch 1 sub ep 83\n",
      "epoch 1 sub ep 84\n",
      "epoch 1 sub ep 85\n",
      "epoch 1 sub ep 86\n",
      "epoch 1 sub ep 87\n",
      "epoch 1 sub ep 88\n",
      "epoch 1 sub ep 89\n",
      "epoch 1 sub ep 90\n",
      "epoch 1 sub ep 91\n",
      "epoch 1 sub ep 92\n",
      "epoch 1 sub ep 93\n",
      "epoch 1 sub ep 94\n",
      "epoch 1 sub ep 95\n",
      "epoch 1 sub ep 96\n",
      "epoch 1 sub ep 97\n",
      "epoch 1 sub ep 98\n",
      "epoch 1 sub ep 99\n",
      "epoch 1 sub ep 100\n",
      "epoch 1 sub ep 101\n",
      "epoch 1 sub ep 102\n",
      "epoch 1 sub ep 103\n",
      "epoch 1 sub ep 104\n",
      "epoch 1 sub ep 105\n",
      "epoch 1 sub ep 106\n",
      "epoch 1 sub ep 107\n",
      "epoch 1 sub ep 108\n",
      "epoch 1 sub ep 109\n",
      "epoch 1 sub ep 110\n",
      "epoch 1 sub ep 111\n",
      "epoch 1 sub ep 112\n",
      "epoch 1 sub ep 113\n",
      "epoch 1 sub ep 114\n",
      "epoch 1 sub ep 115\n",
      "epoch 1 sub ep 116\n",
      "epoch 1 sub ep 117\n",
      "epoch 1 sub ep 118\n",
      "epoch 1 sub ep 119\n",
      "epoch 1 sub ep 120\n",
      "epoch 1 sub ep 121\n",
      "epoch 1 sub ep 122\n",
      "epoch 1 sub ep 123\n",
      "epoch 1 sub ep 124\n",
      "epoch 1 sub ep 125\n",
      "epoch 1 sub ep 126\n",
      "epoch 1 sub ep 127\n",
      "epoch 1 sub ep 128\n",
      "epoch 1 sub ep 129\n",
      "epoch 1 sub ep 130\n",
      "epoch 1 sub ep 131\n",
      "epoch 1 sub ep 132\n",
      "epoch 1 sub ep 133\n",
      "epoch 1 sub ep 134\n",
      "epoch 1 sub ep 135\n",
      "epoch 1 sub ep 136\n",
      "epoch 1 sub ep 137\n",
      "epoch 1 sub ep 138\n",
      "epoch 1 sub ep 139\n",
      "epoch 1 sub ep 140\n",
      "epoch 1 sub ep 141\n",
      "epoch 1 sub ep 142\n",
      "epoch 1 sub ep 143\n",
      "epoch 1 sub ep 144\n",
      "epoch 1 sub ep 145\n",
      "epoch 1 sub ep 146\n",
      "epoch 1 sub ep 147\n",
      "epoch 1 sub ep 148\n",
      "epoch 1 sub ep 149\n",
      "epoch 1 sub ep 150\n",
      "epoch 1 sub ep 151\n",
      "epoch 1 sub ep 152\n",
      "epoch 1 sub ep 153\n",
      "epoch 1 sub ep 154\n",
      "epoch 1 sub ep 155\n",
      "epoch 1 sub ep 156\n",
      "epoch 1 sub ep 157\n",
      "epoch 1 sub ep 158\n",
      "epoch 1 sub ep 159\n",
      "epoch 1 sub ep 160\n",
      "epoch 1 sub ep 161\n",
      "epoch 1 sub ep 162\n",
      "epoch 1 sub ep 163\n",
      "epoch 1 sub ep 164\n",
      "epoch 1 sub ep 165\n",
      "epoch 1 sub ep 166\n",
      "epoch 1 sub ep 167\n",
      "epoch 1 sub ep 168\n",
      "epoch 1 sub ep 169\n",
      "epoch 1 sub ep 170\n",
      "epoch 1 sub ep 171\n",
      "epoch 1 sub ep 172\n",
      "epoch 1 sub ep 173\n",
      "epoch 1 sub ep 174\n",
      "epoch 1 sub ep 175\n",
      "epoch 1 sub ep 176\n",
      "epoch 1 sub ep 177\n",
      "epoch 1 sub ep 178\n",
      "epoch 1 sub ep 179\n",
      "epoch 1 sub ep 180\n",
      "epoch 1 sub ep 181\n",
      "epoch 1 sub ep 182\n",
      "epoch 1 sub ep 183\n",
      "epoch 1 sub ep 184\n",
      "epoch 1 sub ep 185\n",
      "epoch 1 sub ep 186\n",
      "epoch 1 sub ep 187\n",
      "epoch 1 sub ep 188\n",
      "epoch 1 sub ep 189\n",
      "epoch 1 sub ep 190\n",
      "epoch 1 sub ep 191\n",
      "epoch 1 sub ep 192\n",
      "epoch 1 sub ep 193\n",
      "epoch 1 sub ep 194\n",
      "epoch 1 sub ep 195\n",
      "epoch 1 sub ep 196\n",
      "epoch 1 sub ep 197\n",
      "epoch 1 sub ep 198\n",
      "epoch 1 sub ep 199\n",
      "epoch 1 sub ep 200\n",
      "epoch 1 sub ep 201\n",
      "epoch 1 sub ep 202\n",
      "epoch 1 sub ep 203\n",
      "epoch 1 sub ep 204\n",
      "epoch 1 sub ep 205\n",
      "epoch 1 sub ep 206\n",
      "epoch 1 sub ep 207\n",
      "epoch 1 sub ep 208\n",
      "epoch 1 sub ep 209\n",
      "epoch 1 sub ep 210\n",
      "epoch 1 sub ep 211\n",
      "epoch 1 sub ep 212\n",
      "epoch 1 sub ep 213\n",
      "epoch 1 sub ep 214\n",
      "epoch 1 sub ep 215\n",
      "epoch 1 sub ep 216\n",
      "epoch 1 sub ep 217\n",
      "epoch 1 sub ep 218\n",
      "epoch 1 sub ep 219\n",
      "epoch 1 sub ep 220\n",
      "epoch 1 sub ep 221\n",
      "epoch 1 sub ep 222\n",
      "epoch 1 sub ep 223\n",
      "epoch 1 sub ep 224\n",
      "epoch 1 sub ep 225\n",
      "epoch 1 sub ep 226\n",
      "epoch 1 sub ep 227\n",
      "epoch 1 sub ep 228\n",
      "epoch 1 sub ep 229\n",
      "epoch 1 sub ep 230\n",
      "epoch 1 sub ep 231\n",
      "epoch 1 sub ep 232\n",
      "epoch 1 sub ep 233\n",
      "epoch 1 sub ep 234\n",
      "epoch 1 sub ep 235\n",
      "epoch 1 sub ep 236\n",
      "epoch 1 sub ep 237\n",
      "epoch 1 sub ep 238\n",
      "epoch 1 sub ep 239\n",
      "epoch 1 sub ep 240\n",
      "epoch 1 sub ep 241\n",
      "epoch 1 sub ep 242\n",
      "epoch 1 sub ep 243\n",
      "epoch 1 sub ep 244\n",
      "epoch 1 sub ep 245\n",
      "epoch 1 sub ep 246\n",
      "epoch 1 sub ep 247\n",
      "epoch 1 sub ep 248\n",
      "epoch 1 sub ep 249\n",
      "epoch 1 sub ep 250\n",
      "Epoch 2: Train Acc = 94.02%, Val Acc = 93.53%\n",
      "Epch no 2\n",
      "epoch 2 sub ep 1\n",
      "epoch 2 sub ep 2\n",
      "epoch 2 sub ep 3\n",
      "epoch 2 sub ep 4\n",
      "epoch 2 sub ep 5\n",
      "epoch 2 sub ep 6\n",
      "epoch 2 sub ep 7\n",
      "epoch 2 sub ep 8\n",
      "epoch 2 sub ep 9\n",
      "epoch 2 sub ep 10\n",
      "epoch 2 sub ep 11\n",
      "epoch 2 sub ep 12\n",
      "epoch 2 sub ep 13\n",
      "epoch 2 sub ep 14\n",
      "epoch 2 sub ep 15\n",
      "epoch 2 sub ep 16\n",
      "epoch 2 sub ep 17\n",
      "epoch 2 sub ep 18\n",
      "epoch 2 sub ep 19\n",
      "epoch 2 sub ep 20\n",
      "epoch 2 sub ep 21\n",
      "epoch 2 sub ep 22\n",
      "epoch 2 sub ep 23\n",
      "epoch 2 sub ep 24\n",
      "epoch 2 sub ep 25\n",
      "epoch 2 sub ep 26\n",
      "epoch 2 sub ep 27\n",
      "epoch 2 sub ep 28\n",
      "epoch 2 sub ep 29\n",
      "epoch 2 sub ep 30\n",
      "epoch 2 sub ep 31\n",
      "epoch 2 sub ep 32\n",
      "epoch 2 sub ep 33\n",
      "epoch 2 sub ep 34\n",
      "epoch 2 sub ep 35\n",
      "epoch 2 sub ep 36\n",
      "epoch 2 sub ep 37\n",
      "epoch 2 sub ep 38\n",
      "epoch 2 sub ep 39\n",
      "epoch 2 sub ep 40\n",
      "epoch 2 sub ep 41\n",
      "epoch 2 sub ep 42\n",
      "epoch 2 sub ep 43\n",
      "epoch 2 sub ep 44\n",
      "epoch 2 sub ep 45\n",
      "epoch 2 sub ep 46\n",
      "epoch 2 sub ep 47\n",
      "epoch 2 sub ep 48\n",
      "⚠️ Skipping corrupted image: real\\10017.jpg\n",
      "epoch 2 sub ep 49\n",
      "epoch 2 sub ep 50\n",
      "epoch 2 sub ep 51\n",
      "epoch 2 sub ep 52\n",
      "epoch 2 sub ep 53\n",
      "epoch 2 sub ep 54\n",
      "epoch 2 sub ep 55\n",
      "epoch 2 sub ep 56\n",
      "epoch 2 sub ep 57\n",
      "epoch 2 sub ep 58\n",
      "epoch 2 sub ep 59\n",
      "epoch 2 sub ep 60\n",
      "epoch 2 sub ep 61\n",
      "epoch 2 sub ep 62\n",
      "epoch 2 sub ep 63\n",
      "epoch 2 sub ep 64\n",
      "epoch 2 sub ep 65\n",
      "epoch 2 sub ep 66\n",
      "epoch 2 sub ep 67\n",
      "epoch 2 sub ep 68\n",
      "epoch 2 sub ep 69\n",
      "epoch 2 sub ep 70\n",
      "epoch 2 sub ep 71\n",
      "epoch 2 sub ep 72\n",
      "epoch 2 sub ep 73\n",
      "epoch 2 sub ep 74\n",
      "epoch 2 sub ep 75\n",
      "epoch 2 sub ep 76\n",
      "epoch 2 sub ep 77\n",
      "epoch 2 sub ep 78\n",
      "epoch 2 sub ep 79\n",
      "epoch 2 sub ep 80\n",
      "epoch 2 sub ep 81\n",
      "epoch 2 sub ep 82\n",
      "epoch 2 sub ep 83\n",
      "epoch 2 sub ep 84\n",
      "epoch 2 sub ep 85\n",
      "epoch 2 sub ep 86\n",
      "epoch 2 sub ep 87\n",
      "epoch 2 sub ep 88\n",
      "epoch 2 sub ep 89\n",
      "epoch 2 sub ep 90\n",
      "epoch 2 sub ep 91\n",
      "epoch 2 sub ep 92\n",
      "epoch 2 sub ep 93\n",
      "epoch 2 sub ep 94\n",
      "epoch 2 sub ep 95\n",
      "epoch 2 sub ep 96\n",
      "epoch 2 sub ep 97\n",
      "epoch 2 sub ep 98\n",
      "epoch 2 sub ep 99\n",
      "epoch 2 sub ep 100\n",
      "epoch 2 sub ep 101\n",
      "epoch 2 sub ep 102\n",
      "epoch 2 sub ep 103\n",
      "epoch 2 sub ep 104\n",
      "epoch 2 sub ep 105\n",
      "epoch 2 sub ep 106\n",
      "epoch 2 sub ep 107\n",
      "epoch 2 sub ep 108\n",
      "epoch 2 sub ep 109\n",
      "epoch 2 sub ep 110\n",
      "epoch 2 sub ep 111\n",
      "epoch 2 sub ep 112\n",
      "epoch 2 sub ep 113\n",
      "epoch 2 sub ep 114\n",
      "epoch 2 sub ep 115\n",
      "epoch 2 sub ep 116\n",
      "epoch 2 sub ep 117\n",
      "epoch 2 sub ep 118\n",
      "epoch 2 sub ep 119\n",
      "epoch 2 sub ep 120\n",
      "epoch 2 sub ep 121\n",
      "epoch 2 sub ep 122\n",
      "epoch 2 sub ep 123\n",
      "epoch 2 sub ep 124\n",
      "epoch 2 sub ep 125\n",
      "epoch 2 sub ep 126\n",
      "epoch 2 sub ep 127\n",
      "epoch 2 sub ep 128\n",
      "epoch 2 sub ep 129\n",
      "epoch 2 sub ep 130\n",
      "epoch 2 sub ep 131\n",
      "epoch 2 sub ep 132\n",
      "epoch 2 sub ep 133\n",
      "epoch 2 sub ep 134\n",
      "epoch 2 sub ep 135\n",
      "epoch 2 sub ep 136\n",
      "epoch 2 sub ep 137\n",
      "epoch 2 sub ep 138\n",
      "epoch 2 sub ep 139\n",
      "epoch 2 sub ep 140\n",
      "epoch 2 sub ep 141\n",
      "epoch 2 sub ep 142\n",
      "epoch 2 sub ep 143\n",
      "epoch 2 sub ep 144\n",
      "epoch 2 sub ep 145\n",
      "epoch 2 sub ep 146\n",
      "epoch 2 sub ep 147\n",
      "epoch 2 sub ep 148\n",
      "epoch 2 sub ep 149\n",
      "epoch 2 sub ep 150\n",
      "epoch 2 sub ep 151\n",
      "epoch 2 sub ep 152\n",
      "epoch 2 sub ep 153\n",
      "epoch 2 sub ep 154\n",
      "epoch 2 sub ep 155\n",
      "epoch 2 sub ep 156\n",
      "epoch 2 sub ep 157\n",
      "epoch 2 sub ep 158\n",
      "epoch 2 sub ep 159\n",
      "epoch 2 sub ep 160\n",
      "epoch 2 sub ep 161\n",
      "epoch 2 sub ep 162\n",
      "epoch 2 sub ep 163\n",
      "epoch 2 sub ep 164\n",
      "epoch 2 sub ep 165\n",
      "epoch 2 sub ep 166\n",
      "epoch 2 sub ep 167\n",
      "epoch 2 sub ep 168\n",
      "epoch 2 sub ep 169\n",
      "epoch 2 sub ep 170\n",
      "epoch 2 sub ep 171\n",
      "epoch 2 sub ep 172\n",
      "epoch 2 sub ep 173\n",
      "epoch 2 sub ep 174\n",
      "epoch 2 sub ep 175\n",
      "epoch 2 sub ep 176\n",
      "epoch 2 sub ep 177\n",
      "epoch 2 sub ep 178\n",
      "epoch 2 sub ep 179\n",
      "epoch 2 sub ep 180\n",
      "epoch 2 sub ep 181\n",
      "epoch 2 sub ep 182\n",
      "epoch 2 sub ep 183\n",
      "epoch 2 sub ep 184\n",
      "epoch 2 sub ep 185\n",
      "epoch 2 sub ep 186\n",
      "epoch 2 sub ep 187\n",
      "epoch 2 sub ep 188\n",
      "epoch 2 sub ep 189\n",
      "epoch 2 sub ep 190\n",
      "epoch 2 sub ep 191\n",
      "epoch 2 sub ep 192\n",
      "epoch 2 sub ep 193\n",
      "epoch 2 sub ep 194\n",
      "epoch 2 sub ep 195\n",
      "epoch 2 sub ep 196\n",
      "epoch 2 sub ep 197\n",
      "epoch 2 sub ep 198\n",
      "epoch 2 sub ep 199\n",
      "epoch 2 sub ep 200\n",
      "epoch 2 sub ep 201\n",
      "epoch 2 sub ep 202\n",
      "epoch 2 sub ep 203\n",
      "epoch 2 sub ep 204\n",
      "epoch 2 sub ep 205\n",
      "epoch 2 sub ep 206\n",
      "epoch 2 sub ep 207\n",
      "epoch 2 sub ep 208\n",
      "epoch 2 sub ep 209\n",
      "epoch 2 sub ep 210\n",
      "epoch 2 sub ep 211\n",
      "epoch 2 sub ep 212\n",
      "epoch 2 sub ep 213\n",
      "epoch 2 sub ep 214\n",
      "epoch 2 sub ep 215\n",
      "epoch 2 sub ep 216\n",
      "epoch 2 sub ep 217\n",
      "epoch 2 sub ep 218\n",
      "epoch 2 sub ep 219\n",
      "epoch 2 sub ep 220\n",
      "epoch 2 sub ep 221\n",
      "epoch 2 sub ep 222\n",
      "epoch 2 sub ep 223\n",
      "epoch 2 sub ep 224\n",
      "epoch 2 sub ep 225\n",
      "epoch 2 sub ep 226\n",
      "epoch 2 sub ep 227\n",
      "epoch 2 sub ep 228\n",
      "epoch 2 sub ep 229\n",
      "epoch 2 sub ep 230\n",
      "epoch 2 sub ep 231\n",
      "epoch 2 sub ep 232\n",
      "epoch 2 sub ep 233\n",
      "epoch 2 sub ep 234\n",
      "epoch 2 sub ep 235\n",
      "epoch 2 sub ep 236\n",
      "epoch 2 sub ep 237\n",
      "epoch 2 sub ep 238\n",
      "epoch 2 sub ep 239\n",
      "epoch 2 sub ep 240\n",
      "epoch 2 sub ep 241\n",
      "epoch 2 sub ep 242\n",
      "epoch 2 sub ep 243\n",
      "epoch 2 sub ep 244\n",
      "epoch 2 sub ep 245\n",
      "epoch 2 sub ep 246\n",
      "epoch 2 sub ep 247\n",
      "epoch 2 sub ep 248\n",
      "epoch 2 sub ep 249\n",
      "epoch 2 sub ep 250\n",
      "Epoch 3: Train Acc = 95.34%, Val Acc = 93.95%\n",
      "Epch no 3\n",
      "epoch 3 sub ep 1\n",
      "epoch 3 sub ep 2\n",
      "epoch 3 sub ep 3\n",
      "epoch 3 sub ep 4\n",
      "epoch 3 sub ep 5\n",
      "epoch 3 sub ep 6\n",
      "epoch 3 sub ep 7\n",
      "epoch 3 sub ep 8\n",
      "epoch 3 sub ep 9\n",
      "epoch 3 sub ep 10\n",
      "epoch 3 sub ep 11\n",
      "epoch 3 sub ep 12\n",
      "epoch 3 sub ep 13\n",
      "epoch 3 sub ep 14\n",
      "epoch 3 sub ep 15\n",
      "epoch 3 sub ep 16\n",
      "epoch 3 sub ep 17\n",
      "epoch 3 sub ep 18\n",
      "epoch 3 sub ep 19\n",
      "epoch 3 sub ep 20\n",
      "epoch 3 sub ep 21\n",
      "epoch 3 sub ep 22\n",
      "epoch 3 sub ep 23\n",
      "epoch 3 sub ep 24\n",
      "epoch 3 sub ep 25\n",
      "epoch 3 sub ep 26\n",
      "epoch 3 sub ep 27\n",
      "epoch 3 sub ep 28\n",
      "epoch 3 sub ep 29\n",
      "epoch 3 sub ep 30\n",
      "epoch 3 sub ep 31\n",
      "epoch 3 sub ep 32\n",
      "epoch 3 sub ep 33\n",
      "epoch 3 sub ep 34\n",
      "epoch 3 sub ep 35\n",
      "epoch 3 sub ep 36\n",
      "epoch 3 sub ep 37\n",
      "epoch 3 sub ep 38\n",
      "epoch 3 sub ep 39\n",
      "epoch 3 sub ep 40\n",
      "epoch 3 sub ep 41\n",
      "epoch 3 sub ep 42\n",
      "epoch 3 sub ep 43\n",
      "epoch 3 sub ep 44\n",
      "epoch 3 sub ep 45\n",
      "epoch 3 sub ep 46\n",
      "epoch 3 sub ep 47\n",
      "epoch 3 sub ep 48\n",
      "epoch 3 sub ep 49\n",
      "epoch 3 sub ep 50\n",
      "epoch 3 sub ep 51\n",
      "epoch 3 sub ep 52\n",
      "epoch 3 sub ep 53\n",
      "epoch 3 sub ep 54\n",
      "epoch 3 sub ep 55\n",
      "epoch 3 sub ep 56\n",
      "epoch 3 sub ep 57\n",
      "epoch 3 sub ep 58\n",
      "epoch 3 sub ep 59\n",
      "epoch 3 sub ep 60\n",
      "epoch 3 sub ep 61\n",
      "epoch 3 sub ep 62\n",
      "epoch 3 sub ep 63\n",
      "⚠️ Skipping corrupted image: real\\10017.jpg\n",
      "epoch 3 sub ep 64\n",
      "epoch 3 sub ep 65\n",
      "epoch 3 sub ep 66\n",
      "epoch 3 sub ep 67\n",
      "epoch 3 sub ep 68\n",
      "epoch 3 sub ep 69\n",
      "epoch 3 sub ep 70\n",
      "epoch 3 sub ep 71\n",
      "epoch 3 sub ep 72\n",
      "epoch 3 sub ep 73\n",
      "epoch 3 sub ep 74\n",
      "epoch 3 sub ep 75\n",
      "epoch 3 sub ep 76\n",
      "epoch 3 sub ep 77\n",
      "epoch 3 sub ep 78\n",
      "epoch 3 sub ep 79\n",
      "epoch 3 sub ep 80\n",
      "epoch 3 sub ep 81\n",
      "epoch 3 sub ep 82\n",
      "epoch 3 sub ep 83\n",
      "epoch 3 sub ep 84\n",
      "epoch 3 sub ep 85\n",
      "epoch 3 sub ep 86\n",
      "epoch 3 sub ep 87\n",
      "epoch 3 sub ep 88\n",
      "epoch 3 sub ep 89\n",
      "epoch 3 sub ep 90\n",
      "epoch 3 sub ep 91\n",
      "epoch 3 sub ep 92\n",
      "epoch 3 sub ep 93\n",
      "epoch 3 sub ep 94\n",
      "epoch 3 sub ep 95\n",
      "epoch 3 sub ep 96\n",
      "epoch 3 sub ep 97\n",
      "epoch 3 sub ep 98\n",
      "epoch 3 sub ep 99\n",
      "epoch 3 sub ep 100\n",
      "epoch 3 sub ep 101\n",
      "epoch 3 sub ep 102\n",
      "epoch 3 sub ep 103\n",
      "epoch 3 sub ep 104\n",
      "epoch 3 sub ep 105\n",
      "epoch 3 sub ep 106\n",
      "epoch 3 sub ep 107\n",
      "epoch 3 sub ep 108\n",
      "epoch 3 sub ep 109\n",
      "epoch 3 sub ep 110\n",
      "epoch 3 sub ep 111\n",
      "epoch 3 sub ep 112\n",
      "epoch 3 sub ep 113\n",
      "epoch 3 sub ep 114\n",
      "epoch 3 sub ep 115\n",
      "epoch 3 sub ep 116\n",
      "epoch 3 sub ep 117\n",
      "epoch 3 sub ep 118\n",
      "epoch 3 sub ep 119\n",
      "epoch 3 sub ep 120\n",
      "epoch 3 sub ep 121\n",
      "epoch 3 sub ep 122\n",
      "epoch 3 sub ep 123\n",
      "epoch 3 sub ep 124\n",
      "epoch 3 sub ep 125\n",
      "epoch 3 sub ep 126\n",
      "epoch 3 sub ep 127\n",
      "epoch 3 sub ep 128\n",
      "epoch 3 sub ep 129\n",
      "epoch 3 sub ep 130\n",
      "epoch 3 sub ep 131\n",
      "epoch 3 sub ep 132\n",
      "epoch 3 sub ep 133\n",
      "epoch 3 sub ep 134\n",
      "epoch 3 sub ep 135\n",
      "epoch 3 sub ep 136\n",
      "epoch 3 sub ep 137\n",
      "epoch 3 sub ep 138\n",
      "epoch 3 sub ep 139\n",
      "epoch 3 sub ep 140\n",
      "epoch 3 sub ep 141\n",
      "epoch 3 sub ep 142\n",
      "epoch 3 sub ep 143\n",
      "epoch 3 sub ep 144\n",
      "epoch 3 sub ep 145\n",
      "epoch 3 sub ep 146\n",
      "epoch 3 sub ep 147\n",
      "epoch 3 sub ep 148\n",
      "epoch 3 sub ep 149\n",
      "epoch 3 sub ep 150\n",
      "epoch 3 sub ep 151\n",
      "epoch 3 sub ep 152\n",
      "epoch 3 sub ep 153\n",
      "epoch 3 sub ep 154\n",
      "epoch 3 sub ep 155\n",
      "epoch 3 sub ep 156\n",
      "epoch 3 sub ep 157\n",
      "epoch 3 sub ep 158\n",
      "epoch 3 sub ep 159\n",
      "epoch 3 sub ep 160\n",
      "epoch 3 sub ep 161\n",
      "epoch 3 sub ep 162\n",
      "epoch 3 sub ep 163\n",
      "epoch 3 sub ep 164\n",
      "epoch 3 sub ep 165\n",
      "epoch 3 sub ep 166\n",
      "epoch 3 sub ep 167\n",
      "epoch 3 sub ep 168\n",
      "epoch 3 sub ep 169\n",
      "epoch 3 sub ep 170\n",
      "epoch 3 sub ep 171\n",
      "epoch 3 sub ep 172\n",
      "epoch 3 sub ep 173\n",
      "epoch 3 sub ep 174\n",
      "epoch 3 sub ep 175\n",
      "epoch 3 sub ep 176\n",
      "epoch 3 sub ep 177\n",
      "epoch 3 sub ep 178\n",
      "epoch 3 sub ep 179\n",
      "epoch 3 sub ep 180\n",
      "epoch 3 sub ep 181\n",
      "epoch 3 sub ep 182\n",
      "epoch 3 sub ep 183\n",
      "epoch 3 sub ep 184\n",
      "epoch 3 sub ep 185\n",
      "epoch 3 sub ep 186\n",
      "epoch 3 sub ep 187\n",
      "epoch 3 sub ep 188\n",
      "epoch 3 sub ep 189\n",
      "epoch 3 sub ep 190\n",
      "epoch 3 sub ep 191\n",
      "epoch 3 sub ep 192\n",
      "epoch 3 sub ep 193\n",
      "epoch 3 sub ep 194\n",
      "epoch 3 sub ep 195\n",
      "epoch 3 sub ep 196\n",
      "epoch 3 sub ep 197\n",
      "epoch 3 sub ep 198\n",
      "epoch 3 sub ep 199\n",
      "epoch 3 sub ep 200\n",
      "epoch 3 sub ep 201\n",
      "epoch 3 sub ep 202\n",
      "epoch 3 sub ep 203\n",
      "epoch 3 sub ep 204\n",
      "epoch 3 sub ep 205\n",
      "epoch 3 sub ep 206\n",
      "epoch 3 sub ep 207\n",
      "epoch 3 sub ep 208\n",
      "epoch 3 sub ep 209\n",
      "epoch 3 sub ep 210\n",
      "epoch 3 sub ep 211\n",
      "epoch 3 sub ep 212\n",
      "epoch 3 sub ep 213\n",
      "epoch 3 sub ep 214\n",
      "epoch 3 sub ep 215\n",
      "epoch 3 sub ep 216\n",
      "epoch 3 sub ep 217\n",
      "epoch 3 sub ep 218\n",
      "epoch 3 sub ep 219\n",
      "epoch 3 sub ep 220\n",
      "epoch 3 sub ep 221\n",
      "epoch 3 sub ep 222\n",
      "epoch 3 sub ep 223\n",
      "epoch 3 sub ep 224\n",
      "epoch 3 sub ep 225\n",
      "epoch 3 sub ep 226\n",
      "epoch 3 sub ep 227\n",
      "epoch 3 sub ep 228\n",
      "epoch 3 sub ep 229\n",
      "epoch 3 sub ep 230\n",
      "epoch 3 sub ep 231\n",
      "epoch 3 sub ep 232\n",
      "epoch 3 sub ep 233\n",
      "epoch 3 sub ep 234\n",
      "epoch 3 sub ep 235\n",
      "epoch 3 sub ep 236\n",
      "epoch 3 sub ep 237\n",
      "epoch 3 sub ep 238\n",
      "epoch 3 sub ep 239\n",
      "epoch 3 sub ep 240\n",
      "epoch 3 sub ep 241\n",
      "epoch 3 sub ep 242\n",
      "epoch 3 sub ep 243\n",
      "epoch 3 sub ep 244\n",
      "epoch 3 sub ep 245\n",
      "epoch 3 sub ep 246\n",
      "epoch 3 sub ep 247\n",
      "epoch 3 sub ep 248\n",
      "epoch 3 sub ep 249\n",
      "epoch 3 sub ep 250\n",
      "Epoch 4: Train Acc = 96.16%, Val Acc = 93.65%\n",
      "Epch no 4\n",
      "epoch 4 sub ep 1\n",
      "epoch 4 sub ep 2\n",
      "epoch 4 sub ep 3\n",
      "epoch 4 sub ep 4\n",
      "epoch 4 sub ep 5\n",
      "epoch 4 sub ep 6\n",
      "epoch 4 sub ep 7\n",
      "epoch 4 sub ep 8\n",
      "epoch 4 sub ep 9\n",
      "epoch 4 sub ep 10\n",
      "epoch 4 sub ep 11\n",
      "epoch 4 sub ep 12\n",
      "epoch 4 sub ep 13\n",
      "epoch 4 sub ep 14\n",
      "epoch 4 sub ep 15\n",
      "epoch 4 sub ep 16\n",
      "epoch 4 sub ep 17\n",
      "epoch 4 sub ep 18\n",
      "epoch 4 sub ep 19\n",
      "epoch 4 sub ep 20\n",
      "epoch 4 sub ep 21\n",
      "epoch 4 sub ep 22\n",
      "epoch 4 sub ep 23\n",
      "epoch 4 sub ep 24\n",
      "epoch 4 sub ep 25\n",
      "epoch 4 sub ep 26\n",
      "epoch 4 sub ep 27\n",
      "epoch 4 sub ep 28\n",
      "epoch 4 sub ep 29\n",
      "epoch 4 sub ep 30\n",
      "epoch 4 sub ep 31\n",
      "epoch 4 sub ep 32\n",
      "epoch 4 sub ep 33\n",
      "epoch 4 sub ep 34\n",
      "epoch 4 sub ep 35\n",
      "epoch 4 sub ep 36\n",
      "epoch 4 sub ep 37\n",
      "epoch 4 sub ep 38\n",
      "epoch 4 sub ep 39\n",
      "epoch 4 sub ep 40\n",
      "epoch 4 sub ep 41\n",
      "epoch 4 sub ep 42\n",
      "epoch 4 sub ep 43\n",
      "epoch 4 sub ep 44\n",
      "epoch 4 sub ep 45\n",
      "epoch 4 sub ep 46\n",
      "epoch 4 sub ep 47\n",
      "epoch 4 sub ep 48\n",
      "epoch 4 sub ep 49\n",
      "epoch 4 sub ep 50\n",
      "epoch 4 sub ep 51\n",
      "epoch 4 sub ep 52\n",
      "epoch 4 sub ep 53\n",
      "epoch 4 sub ep 54\n",
      "epoch 4 sub ep 55\n",
      "epoch 4 sub ep 56\n",
      "epoch 4 sub ep 57\n",
      "epoch 4 sub ep 58\n",
      "epoch 4 sub ep 59\n",
      "epoch 4 sub ep 60\n",
      "epoch 4 sub ep 61\n",
      "epoch 4 sub ep 62\n",
      "epoch 4 sub ep 63\n",
      "epoch 4 sub ep 64\n",
      "epoch 4 sub ep 65\n",
      "epoch 4 sub ep 66\n",
      "epoch 4 sub ep 67\n",
      "epoch 4 sub ep 68\n",
      "epoch 4 sub ep 69\n",
      "epoch 4 sub ep 70\n",
      "epoch 4 sub ep 71\n",
      "epoch 4 sub ep 72\n",
      "epoch 4 sub ep 73\n",
      "epoch 4 sub ep 74\n",
      "epoch 4 sub ep 75\n",
      "epoch 4 sub ep 76\n",
      "epoch 4 sub ep 77\n",
      "epoch 4 sub ep 78\n",
      "epoch 4 sub ep 79\n",
      "epoch 4 sub ep 80\n",
      "epoch 4 sub ep 81\n",
      "epoch 4 sub ep 82\n",
      "epoch 4 sub ep 83\n",
      "epoch 4 sub ep 84\n",
      "epoch 4 sub ep 85\n",
      "epoch 4 sub ep 86\n",
      "epoch 4 sub ep 87\n",
      "epoch 4 sub ep 88\n",
      "epoch 4 sub ep 89\n",
      "epoch 4 sub ep 90\n",
      "epoch 4 sub ep 91\n",
      "epoch 4 sub ep 92\n",
      "epoch 4 sub ep 93\n",
      "epoch 4 sub ep 94\n",
      "epoch 4 sub ep 95\n",
      "epoch 4 sub ep 96\n",
      "epoch 4 sub ep 97\n",
      "epoch 4 sub ep 98\n",
      "epoch 4 sub ep 99\n",
      "epoch 4 sub ep 100\n",
      "epoch 4 sub ep 101\n",
      "epoch 4 sub ep 102\n",
      "epoch 4 sub ep 103\n",
      "epoch 4 sub ep 104\n",
      "epoch 4 sub ep 105\n",
      "epoch 4 sub ep 106\n",
      "epoch 4 sub ep 107\n",
      "epoch 4 sub ep 108\n",
      "epoch 4 sub ep 109\n",
      "epoch 4 sub ep 110\n",
      "epoch 4 sub ep 111\n",
      "epoch 4 sub ep 112\n",
      "epoch 4 sub ep 113\n",
      "epoch 4 sub ep 114\n",
      "epoch 4 sub ep 115\n",
      "epoch 4 sub ep 116\n",
      "epoch 4 sub ep 117\n",
      "epoch 4 sub ep 118\n",
      "epoch 4 sub ep 119\n",
      "epoch 4 sub ep 120\n",
      "epoch 4 sub ep 121\n",
      "epoch 4 sub ep 122\n",
      "epoch 4 sub ep 123\n",
      "epoch 4 sub ep 124\n",
      "epoch 4 sub ep 125\n",
      "epoch 4 sub ep 126\n",
      "epoch 4 sub ep 127\n",
      "epoch 4 sub ep 128\n",
      "epoch 4 sub ep 129\n",
      "epoch 4 sub ep 130\n",
      "epoch 4 sub ep 131\n",
      "epoch 4 sub ep 132\n",
      "epoch 4 sub ep 133\n",
      "epoch 4 sub ep 134\n",
      "epoch 4 sub ep 135\n",
      "epoch 4 sub ep 136\n",
      "epoch 4 sub ep 137\n",
      "epoch 4 sub ep 138\n",
      "epoch 4 sub ep 139\n",
      "epoch 4 sub ep 140\n",
      "epoch 4 sub ep 141\n",
      "epoch 4 sub ep 142\n",
      "epoch 4 sub ep 143\n",
      "epoch 4 sub ep 144\n",
      "epoch 4 sub ep 145\n",
      "epoch 4 sub ep 146\n",
      "epoch 4 sub ep 147\n",
      "epoch 4 sub ep 148\n",
      "epoch 4 sub ep 149\n",
      "epoch 4 sub ep 150\n",
      "epoch 4 sub ep 151\n",
      "epoch 4 sub ep 152\n",
      "epoch 4 sub ep 153\n",
      "epoch 4 sub ep 154\n",
      "epoch 4 sub ep 155\n",
      "epoch 4 sub ep 156\n",
      "epoch 4 sub ep 157\n",
      "epoch 4 sub ep 158\n",
      "epoch 4 sub ep 159\n",
      "epoch 4 sub ep 160\n",
      "epoch 4 sub ep 161\n",
      "epoch 4 sub ep 162\n",
      "epoch 4 sub ep 163\n",
      "⚠️ Skipping corrupted image: real\\10017.jpg\n",
      "epoch 4 sub ep 164\n",
      "epoch 4 sub ep 165\n",
      "epoch 4 sub ep 166\n",
      "epoch 4 sub ep 167\n",
      "epoch 4 sub ep 168\n",
      "epoch 4 sub ep 169\n",
      "epoch 4 sub ep 170\n",
      "epoch 4 sub ep 171\n",
      "epoch 4 sub ep 172\n",
      "epoch 4 sub ep 173\n",
      "epoch 4 sub ep 174\n",
      "epoch 4 sub ep 175\n",
      "epoch 4 sub ep 176\n",
      "epoch 4 sub ep 177\n",
      "epoch 4 sub ep 178\n",
      "epoch 4 sub ep 179\n",
      "epoch 4 sub ep 180\n",
      "epoch 4 sub ep 181\n",
      "epoch 4 sub ep 182\n",
      "epoch 4 sub ep 183\n",
      "epoch 4 sub ep 184\n",
      "epoch 4 sub ep 185\n",
      "epoch 4 sub ep 186\n",
      "epoch 4 sub ep 187\n",
      "epoch 4 sub ep 188\n",
      "epoch 4 sub ep 189\n",
      "epoch 4 sub ep 190\n",
      "epoch 4 sub ep 191\n",
      "epoch 4 sub ep 192\n",
      "epoch 4 sub ep 193\n",
      "epoch 4 sub ep 194\n",
      "epoch 4 sub ep 195\n",
      "epoch 4 sub ep 196\n",
      "epoch 4 sub ep 197\n",
      "epoch 4 sub ep 198\n",
      "epoch 4 sub ep 199\n",
      "epoch 4 sub ep 200\n",
      "epoch 4 sub ep 201\n",
      "epoch 4 sub ep 202\n",
      "epoch 4 sub ep 203\n",
      "epoch 4 sub ep 204\n",
      "epoch 4 sub ep 205\n",
      "epoch 4 sub ep 206\n",
      "epoch 4 sub ep 207\n",
      "epoch 4 sub ep 208\n",
      "epoch 4 sub ep 209\n",
      "epoch 4 sub ep 210\n",
      "epoch 4 sub ep 211\n",
      "epoch 4 sub ep 212\n",
      "epoch 4 sub ep 213\n",
      "epoch 4 sub ep 214\n",
      "epoch 4 sub ep 215\n",
      "epoch 4 sub ep 216\n",
      "epoch 4 sub ep 217\n",
      "epoch 4 sub ep 218\n",
      "epoch 4 sub ep 219\n",
      "epoch 4 sub ep 220\n",
      "epoch 4 sub ep 221\n",
      "epoch 4 sub ep 222\n",
      "epoch 4 sub ep 223\n",
      "epoch 4 sub ep 224\n",
      "epoch 4 sub ep 225\n",
      "epoch 4 sub ep 226\n",
      "epoch 4 sub ep 227\n",
      "epoch 4 sub ep 228\n",
      "epoch 4 sub ep 229\n",
      "epoch 4 sub ep 230\n",
      "epoch 4 sub ep 231\n",
      "epoch 4 sub ep 232\n",
      "epoch 4 sub ep 233\n",
      "epoch 4 sub ep 234\n",
      "epoch 4 sub ep 235\n",
      "epoch 4 sub ep 236\n",
      "epoch 4 sub ep 237\n",
      "epoch 4 sub ep 238\n",
      "epoch 4 sub ep 239\n",
      "epoch 4 sub ep 240\n",
      "epoch 4 sub ep 241\n",
      "epoch 4 sub ep 242\n",
      "epoch 4 sub ep 243\n",
      "epoch 4 sub ep 244\n",
      "epoch 4 sub ep 245\n",
      "epoch 4 sub ep 246\n",
      "epoch 4 sub ep 247\n",
      "epoch 4 sub ep 248\n",
      "epoch 4 sub ep 249\n",
      "epoch 4 sub ep 250\n",
      "Epoch 5: Train Acc = 97.03%, Val Acc = 92.95%\n",
      "Epch no 5\n",
      "epoch 5 sub ep 1\n",
      "epoch 5 sub ep 2\n",
      "epoch 5 sub ep 3\n",
      "epoch 5 sub ep 4\n",
      "epoch 5 sub ep 5\n",
      "epoch 5 sub ep 6\n",
      "epoch 5 sub ep 7\n",
      "epoch 5 sub ep 8\n",
      "epoch 5 sub ep 9\n",
      "epoch 5 sub ep 10\n",
      "epoch 5 sub ep 11\n",
      "epoch 5 sub ep 12\n",
      "epoch 5 sub ep 13\n",
      "epoch 5 sub ep 14\n",
      "epoch 5 sub ep 15\n",
      "epoch 5 sub ep 16\n",
      "epoch 5 sub ep 17\n",
      "epoch 5 sub ep 18\n",
      "epoch 5 sub ep 19\n",
      "epoch 5 sub ep 20\n",
      "epoch 5 sub ep 21\n",
      "epoch 5 sub ep 22\n",
      "epoch 5 sub ep 23\n",
      "epoch 5 sub ep 24\n",
      "epoch 5 sub ep 25\n",
      "epoch 5 sub ep 26\n",
      "epoch 5 sub ep 27\n",
      "epoch 5 sub ep 28\n",
      "epoch 5 sub ep 29\n",
      "epoch 5 sub ep 30\n",
      "epoch 5 sub ep 31\n",
      "epoch 5 sub ep 32\n",
      "epoch 5 sub ep 33\n",
      "epoch 5 sub ep 34\n",
      "epoch 5 sub ep 35\n",
      "epoch 5 sub ep 36\n",
      "epoch 5 sub ep 37\n",
      "epoch 5 sub ep 38\n",
      "epoch 5 sub ep 39\n",
      "epoch 5 sub ep 40\n",
      "epoch 5 sub ep 41\n",
      "epoch 5 sub ep 42\n",
      "epoch 5 sub ep 43\n",
      "epoch 5 sub ep 44\n",
      "epoch 5 sub ep 45\n",
      "epoch 5 sub ep 46\n",
      "epoch 5 sub ep 47\n",
      "epoch 5 sub ep 48\n",
      "epoch 5 sub ep 49\n",
      "epoch 5 sub ep 50\n",
      "epoch 5 sub ep 51\n",
      "epoch 5 sub ep 52\n",
      "epoch 5 sub ep 53\n",
      "epoch 5 sub ep 54\n",
      "epoch 5 sub ep 55\n",
      "epoch 5 sub ep 56\n",
      "epoch 5 sub ep 57\n",
      "epoch 5 sub ep 58\n",
      "epoch 5 sub ep 59\n",
      "epoch 5 sub ep 60\n",
      "epoch 5 sub ep 61\n",
      "epoch 5 sub ep 62\n",
      "epoch 5 sub ep 63\n",
      "epoch 5 sub ep 64\n",
      "epoch 5 sub ep 65\n",
      "epoch 5 sub ep 66\n",
      "epoch 5 sub ep 67\n",
      "epoch 5 sub ep 68\n",
      "epoch 5 sub ep 69\n",
      "epoch 5 sub ep 70\n",
      "epoch 5 sub ep 71\n",
      "epoch 5 sub ep 72\n",
      "epoch 5 sub ep 73\n",
      "epoch 5 sub ep 74\n",
      "epoch 5 sub ep 75\n",
      "epoch 5 sub ep 76\n",
      "epoch 5 sub ep 77\n",
      "epoch 5 sub ep 78\n",
      "epoch 5 sub ep 79\n",
      "epoch 5 sub ep 80\n",
      "epoch 5 sub ep 81\n",
      "epoch 5 sub ep 82\n",
      "epoch 5 sub ep 83\n",
      "epoch 5 sub ep 84\n",
      "epoch 5 sub ep 85\n",
      "epoch 5 sub ep 86\n",
      "epoch 5 sub ep 87\n",
      "epoch 5 sub ep 88\n",
      "epoch 5 sub ep 89\n",
      "epoch 5 sub ep 90\n",
      "epoch 5 sub ep 91\n",
      "epoch 5 sub ep 92\n",
      "epoch 5 sub ep 93\n",
      "epoch 5 sub ep 94\n",
      "epoch 5 sub ep 95\n",
      "epoch 5 sub ep 96\n",
      "epoch 5 sub ep 97\n",
      "epoch 5 sub ep 98\n",
      "epoch 5 sub ep 99\n",
      "epoch 5 sub ep 100\n",
      "epoch 5 sub ep 101\n",
      "epoch 5 sub ep 102\n",
      "epoch 5 sub ep 103\n",
      "epoch 5 sub ep 104\n",
      "epoch 5 sub ep 105\n",
      "epoch 5 sub ep 106\n",
      "epoch 5 sub ep 107\n",
      "epoch 5 sub ep 108\n",
      "epoch 5 sub ep 109\n",
      "epoch 5 sub ep 110\n",
      "epoch 5 sub ep 111\n",
      "epoch 5 sub ep 112\n",
      "epoch 5 sub ep 113\n",
      "epoch 5 sub ep 114\n",
      "epoch 5 sub ep 115\n",
      "epoch 5 sub ep 116\n",
      "epoch 5 sub ep 117\n",
      "epoch 5 sub ep 118\n",
      "epoch 5 sub ep 119\n",
      "epoch 5 sub ep 120\n",
      "epoch 5 sub ep 121\n",
      "epoch 5 sub ep 122\n",
      "epoch 5 sub ep 123\n",
      "epoch 5 sub ep 124\n",
      "epoch 5 sub ep 125\n",
      "epoch 5 sub ep 126\n",
      "epoch 5 sub ep 127\n",
      "epoch 5 sub ep 128\n",
      "epoch 5 sub ep 129\n",
      "epoch 5 sub ep 130\n",
      "epoch 5 sub ep 131\n",
      "epoch 5 sub ep 132\n",
      "epoch 5 sub ep 133\n",
      "epoch 5 sub ep 134\n",
      "epoch 5 sub ep 135\n",
      "epoch 5 sub ep 136\n",
      "epoch 5 sub ep 137\n",
      "epoch 5 sub ep 138\n",
      "epoch 5 sub ep 139\n",
      "epoch 5 sub ep 140\n",
      "epoch 5 sub ep 141\n",
      "epoch 5 sub ep 142\n",
      "epoch 5 sub ep 143\n",
      "epoch 5 sub ep 144\n",
      "epoch 5 sub ep 145\n",
      "epoch 5 sub ep 146\n",
      "epoch 5 sub ep 147\n",
      "epoch 5 sub ep 148\n",
      "epoch 5 sub ep 149\n",
      "epoch 5 sub ep 150\n",
      "epoch 5 sub ep 151\n",
      "epoch 5 sub ep 152\n",
      "epoch 5 sub ep 153\n",
      "epoch 5 sub ep 154\n",
      "epoch 5 sub ep 155\n",
      "epoch 5 sub ep 156\n",
      "epoch 5 sub ep 157\n",
      "epoch 5 sub ep 158\n",
      "epoch 5 sub ep 159\n",
      "epoch 5 sub ep 160\n",
      "epoch 5 sub ep 161\n",
      "epoch 5 sub ep 162\n",
      "epoch 5 sub ep 163\n",
      "epoch 5 sub ep 164\n",
      "epoch 5 sub ep 165\n",
      "epoch 5 sub ep 166\n",
      "epoch 5 sub ep 167\n",
      "epoch 5 sub ep 168\n",
      "epoch 5 sub ep 169\n",
      "epoch 5 sub ep 170\n",
      "epoch 5 sub ep 171\n",
      "epoch 5 sub ep 172\n",
      "epoch 5 sub ep 173\n",
      "epoch 5 sub ep 174\n",
      "epoch 5 sub ep 175\n",
      "epoch 5 sub ep 176\n",
      "epoch 5 sub ep 177\n",
      "epoch 5 sub ep 178\n",
      "epoch 5 sub ep 179\n",
      "epoch 5 sub ep 180\n",
      "epoch 5 sub ep 181\n",
      "epoch 5 sub ep 182\n",
      "epoch 5 sub ep 183\n",
      "epoch 5 sub ep 184\n",
      "epoch 5 sub ep 185\n",
      "epoch 5 sub ep 186\n",
      "epoch 5 sub ep 187\n",
      "epoch 5 sub ep 188\n",
      "epoch 5 sub ep 189\n",
      "epoch 5 sub ep 190\n",
      "epoch 5 sub ep 191\n",
      "epoch 5 sub ep 192\n",
      "epoch 5 sub ep 193\n",
      "epoch 5 sub ep 194\n",
      "epoch 5 sub ep 195\n",
      "epoch 5 sub ep 196\n",
      "epoch 5 sub ep 197\n",
      "epoch 5 sub ep 198\n",
      "epoch 5 sub ep 199\n",
      "epoch 5 sub ep 200\n",
      "epoch 5 sub ep 201\n",
      "epoch 5 sub ep 202\n",
      "epoch 5 sub ep 203\n",
      "epoch 5 sub ep 204\n",
      "epoch 5 sub ep 205\n",
      "epoch 5 sub ep 206\n",
      "epoch 5 sub ep 207\n",
      "epoch 5 sub ep 208\n",
      "epoch 5 sub ep 209\n",
      "epoch 5 sub ep 210\n",
      "epoch 5 sub ep 211\n",
      "epoch 5 sub ep 212\n",
      "epoch 5 sub ep 213\n",
      "epoch 5 sub ep 214\n",
      "epoch 5 sub ep 215\n",
      "epoch 5 sub ep 216\n",
      "epoch 5 sub ep 217\n",
      "epoch 5 sub ep 218\n",
      "epoch 5 sub ep 219\n",
      "epoch 5 sub ep 220\n",
      "epoch 5 sub ep 221\n",
      "epoch 5 sub ep 222\n",
      "epoch 5 sub ep 223\n",
      "epoch 5 sub ep 224\n",
      "epoch 5 sub ep 225\n",
      "epoch 5 sub ep 226\n",
      "epoch 5 sub ep 227\n",
      "epoch 5 sub ep 228\n",
      "epoch 5 sub ep 229\n",
      "epoch 5 sub ep 230\n",
      "epoch 5 sub ep 231\n",
      "epoch 5 sub ep 232\n",
      "epoch 5 sub ep 233\n",
      "epoch 5 sub ep 234\n",
      "epoch 5 sub ep 235\n",
      "epoch 5 sub ep 236\n",
      "epoch 5 sub ep 237\n",
      "epoch 5 sub ep 238\n",
      "epoch 5 sub ep 239\n",
      "epoch 5 sub ep 240\n",
      "epoch 5 sub ep 241\n",
      "epoch 5 sub ep 242\n",
      "epoch 5 sub ep 243\n",
      "epoch 5 sub ep 244\n",
      "epoch 5 sub ep 245\n",
      "⚠️ Skipping corrupted image: real\\10017.jpg\n",
      "epoch 5 sub ep 246\n",
      "epoch 5 sub ep 247\n",
      "epoch 5 sub ep 248\n",
      "epoch 5 sub ep 249\n",
      "epoch 5 sub ep 250\n",
      "Epoch 6: Train Acc = 98.23%, Val Acc = 93.95%\n",
      "⛔ Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_15336\\924277021.py:177: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_3.pth'))\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3402: DecompressionBombWarning: Image size (99991727 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ final_submission.csv saved successfully!\n",
      "   filename  class\n",
      "0     1.jpg      0\n",
      "1    10.jpg      1\n",
      "2   100.jpg      1\n",
      "3  1000.jpg      0\n",
      "4  1001.jpg      1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Build dataset from folders\n",
    "def build_dataframe():\n",
    "    data = []\n",
    "    for label, folder in enumerate(['ai', 'real']):\n",
    "        n = 0\n",
    "        for file in os.listdir(folder):\n",
    "            if n == 10000:\n",
    "                break\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.webp', '.gif')):\n",
    "                n += 1\n",
    "                data.append({'file_name': os.path.join(folder, file), 'label': label})\n",
    "    df = pd.DataFrame(data)\n",
    "    return train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "train_df, val_df = build_dataframe()\n",
    "test_files = os.listdir('test_data/teamspace/studios/this_studio/final_test_renamed')\n",
    "test_df = pd.DataFrame({'file_name': [f'test_data/teamspace/studios/this_studio/final_test_renamed/{x}' for x in test_files]})\n",
    "\n",
    "# Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['file_name']\n",
    "        try:\n",
    "            image = Image.open(path).convert('RGB')\n",
    "        except (OSError, ValueError) as e:\n",
    "            print(f\"⚠️ Skipping corrupted image: {path}\")\n",
    "            # Fallback: Return a black image\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return image, -1\n",
    "        else:\n",
    "            label = int(self.df.iloc[idx]['label'])\n",
    "            return image, label\n",
    "\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Hyperparameters from Optuna\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.000762\n",
    "WEIGHT_DECAY = 0.00038036\n",
    "\n",
    "# DataLoaders (⬅️ Made memory-efficient: num_workers=2 and pin_memory=True)\n",
    "train_dataset = ImageDataset(train_df, transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, transform=test_transform)\n",
    "test_dataset = ImageDataset(test_df, transform=test_transform, is_test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Load pretrained model from saved weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = efficientnet_b0(weights=None)  # ⬅️ No default weights since we are loading our own\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model_2.pth', map_location=device))  # ⬅️ Load from given path\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to eval mode for inference\n",
    "\n",
    "\n",
    "# Freeze all layers except last few (⬅️ Same)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.features[-3:].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training loop with early stopping (⬅️ Added Early Stopping logic)\n",
    "best_val_acc = 0\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(20):\n",
    "    print(f\"Epch no {epoch}\")\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    n =0\n",
    "    for images, labels in train_loader:\n",
    "        n+=1\n",
    "        print(f\"epoch {epoch } sub ep {n}\")\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    torch.save(model.state_dict(), f'best_model_3_epoch {epoch}_train_{train_acc}_val_{val_acc}.pth')\n",
    "    print(f\"Epoch {epoch+1}: Train Acc = {train_acc:.2f}%, Val Acc = {val_acc:.2f}%\")\n",
    "    scheduler.step()\n",
    "\n",
    "    # Free up unused memory (⬅️ Added)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model_3.pth')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"⛔ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model for inference\n",
    "model.load_state_dict(torch.load('best_model_3.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Inference on test data\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({\n",
    "    'filename': test_df['file_name'].apply(lambda x: os.path.basename(x)),\n",
    "    'class': preds\n",
    "})\n",
    "submission.to_csv('final_submission_3.csv', index=False)\n",
    "print(\"✅ final_submission.csv saved successfully!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6814f71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_15336\\19415205.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_3_epoch 2_train_95.3375_val_93.95.pth'))\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:3402: DecompressionBombWarning: Image size (99991727 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ final_submission.csv saved successfully!\n",
      "   filename  class\n",
      "0     1.jpg      0\n",
      "1    10.jpg      1\n",
      "2   100.jpg      1\n",
      "3  1000.jpg      0\n",
      "4  1001.jpg      1\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_3_epoch 2_train_95.3375_val_93.95.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Inference on test data\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({\n",
    "    'filename': test_df['file_name'].apply(lambda x: os.path.basename(x)),\n",
    "    'class': preds\n",
    "})\n",
    "submission.to_csv('final_submission_3_1.csv', index=False)\n",
    "print(\"✅ final_submission.csv saved successfully!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d1bfb6",
   "metadata": {},
   "source": [
    "## Changed the Batch size to 128 and dataset size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de81314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_15336\\2235403929.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_3.pth', map_location=device))  # ⬅️ Load from given path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Build dataset from folders\n",
    "def build_dataframe():\n",
    "    data = []\n",
    "    for label, folder in enumerate(['ai', 'real']):\n",
    "        n = 0\n",
    "        for file in os.listdir(folder):\n",
    "            if n == 30000:\n",
    "                break\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.webp', '.gif')):\n",
    "                n += 1\n",
    "                data.append({'file_name': os.path.join(folder, file), 'label': label})\n",
    "    df = pd.DataFrame(data)\n",
    "    return train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "train_df, val_df = build_dataframe()\n",
    "test_files = os.listdir('test_data/teamspace/studios/this_studio/final_test_renamed')\n",
    "test_df = pd.DataFrame({'file_name': [f'test_data/teamspace/studios/this_studio/final_test_renamed/{x}' for x in test_files]})\n",
    "\n",
    "# Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['file_name']\n",
    "        try:\n",
    "            image = Image.open(path).convert('RGB')\n",
    "        except (OSError, ValueError) as e:\n",
    "            print(f\"⚠️ Skipping corrupted image: {path}\")\n",
    "            # Fallback: Return a black image\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return image, -1\n",
    "        else:\n",
    "            label = int(self.df.iloc[idx]['label'])\n",
    "            return image, label\n",
    "\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Hyperparameters from Optuna\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.000762\n",
    "WEIGHT_DECAY = 0.00038036\n",
    "\n",
    "# DataLoaders (⬅️ Made memory-efficient: num_workers=2 and pin_memory=True)\n",
    "train_dataset = ImageDataset(train_df, transform=train_transform)\n",
    "val_dataset = ImageDataset(val_df, transform=test_transform)\n",
    "test_dataset = ImageDataset(test_df, transform=test_transform, is_test=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Load pretrained model from saved weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = efficientnet_b0(weights=None)  # ⬅️ No default weights since we are loading our own\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model_3.pth', map_location=device))  # ⬅️ Load from given path\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to eval mode for inference\n",
    "\n",
    "\n",
    "# Freeze all layers except last few (⬅️ Same)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.features[-3:].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training loop with early stopping (⬅️ Added Early Stopping logic)\n",
    "best_val_acc = 0\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(20):\n",
    "    print(f\"Epch no {epoch}\")\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    n =0\n",
    "    for images, labels in train_loader:\n",
    "        n+=1\n",
    "        print(f\"epoch {epoch } sub ep {n}\")\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    torch.save(model.state_dict(), f'best_model_4_epoch {epoch}_train_{train_acc}_val_{val_acc}.pth')\n",
    "    print(f\"Epoch {epoch+1}: Train Acc = {train_acc:.2f}%, Val Acc = {val_acc:.2f}%\")\n",
    "    scheduler.step()\n",
    "\n",
    "    # Free up unused memory (⬅️ Added)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model_4.pth')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"⛔ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model for inference\n",
    "model.load_state_dict(torch.load('best_model_4.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Inference on test data\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({\n",
    "    'filename': test_df['file_name'].apply(lambda x: os.path.basename(x)),\n",
    "    'class': preds\n",
    "})\n",
    "submission.to_csv('final_submission_4.csv', index=False)\n",
    "print(\"✅ final_submission.csv saved successfully!\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d785a63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_16944\\190736668.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model_4_epoch 10_train_98.6387198557584_val_91.76958442260886.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Load best model for inference\n",
    "model.load_state_dict(torch.load('best_model_4_epoch 10_train_98.6387198557584_val_91.76958442260886.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Inference on test data\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['file_name'].apply(lambda x: os.path.basename(x)),\n",
    "    'label': preds\n",
    "})\n",
    "submission.to_csv('final_submission_4_5.csv', index=False)\n",
    "print(\"✅ final_submission.csv saved successfully!\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238ae66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
